{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57d5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Activation, Conv2D, Conv2DTranspose, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import Resizing, Rescaling\n",
    "from tf.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea3292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82faed06",
   "metadata": {},
   "source": [
    "# Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    lrelu = LeakyReLU(alpha = 0.2)\n",
    "      \n",
    "    inputs = Input(shape = (256, 256, 1))\n",
    "    \n",
    "    encoder_1 = Conv2D(32, (4, 4), (2, 2), padding = 'same')(inputs)\n",
    "    \n",
    "    encoder_2 = Conv2D(64, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_1)\n",
    "    encoder_2 = BatchNormalization()(encoder_2)\n",
    "    \n",
    "    encoder_3 = Conv2D(128, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_2)\n",
    "    encoder_3 = BatchNormalization()(encoder_3)\n",
    "    \n",
    "    encoder_4 = Conv2D(256, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_3)\n",
    "    encoder_4 = BatchNormalization()(encoder_4)\n",
    "    \n",
    "    encoder_5 = Conv2D(256, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_4)\n",
    "    encoder_5 = BatchNormalization()(encoder_5)\n",
    "    \n",
    "    encoder_6 = Conv2D(256, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_5)\n",
    "    encoder_6 = BatchNormalization()(encoder_6)\n",
    "    \n",
    "    encoder_7 = Conv2D(256, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_6)\n",
    "    encoder_7 = BatchNormalization()(encoder_7)\n",
    "    \n",
    "    encoder_8 = Conv2D(256, (4, 4), (2, 2), activation = lrelu, padding = 'same')(encoder_7)\n",
    "    encoder_8 = BatchNormalization()(encoder_8)\n",
    "    \n",
    "    decoder_8 = Conv2DTranspose(256, (4, 4), (2, 2), activation = 'relu', padding = 'same')(encoder_8)\n",
    "    decoder_8 = BatchNormalization()(decoder_8)\n",
    "    decoder_8 = Dropout(0.5)(decoder_8)\n",
    "    \n",
    "    decoder_7 = Conv2DTranspose(256, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_8, encoder_7], axis = -1))\n",
    "    decoder_7 = BatchNormalization()(decoder_7)\n",
    "    decoder_7 = Dropout(0.5)(decoder_7)\n",
    "    \n",
    "    decoder_6 = Conv2DTranspose(256, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_7, encoder_6], axis = -1))\n",
    "    decoder_6 = BatchNormalization()(decoder_6)\n",
    "    decoder_6 = Dropout(0.5)(decoder_6)\n",
    "    \n",
    "    decoder_5 = Conv2DTranspose(256, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_6, encoder_5], axis = -1))\n",
    "    decoder_5 = BatchNormalization()(decoder_5)\n",
    "    \n",
    "    decoder_4 = Conv2DTranspose(128, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_5, encoder_4], axis = -1))\n",
    "    decoder_4 = BatchNormalization()(decoder_4)\n",
    "    \n",
    "    decoder_3 = Conv2DTranspose(64, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_4, encoder_3], axis = -1))\n",
    "    decoder_3 = BatchNormalization()(decoder_3)\n",
    "    \n",
    "    decoder_2 = Conv2DTranspose(32, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_3, encoder_2], axis = -1))\n",
    "    decoder_2 = BatchNormalization()(decoder_2)\n",
    "    \n",
    "    decoder_1 = Conv2DTranspose(1, (4, 4), (2, 2), activation = 'relu', padding = 'same')\\\n",
    "                        (concatenate([decoder_2, encoder_1], axis = -1))\n",
    "    \n",
    "    outputs = Activation('tanh')(decoder_1)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922aad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    inputs = Input(shape = (256, 256, 2))\n",
    "    lrelu = LeakyReLU(alpha = 0.2)\n",
    "    \n",
    "    layer_1 = Conv2D(32, (4, 4), (2, 2), activation = lrelu, padding = 'same')(inputs)\n",
    "    \n",
    "    layer_2 = Conv2D(64, (4, 4), (2, 2), activation = lrelu, padding = 'same')(layer_1)\n",
    "    layer_2 = BatchNormalization()(layer_2)\n",
    "    \n",
    "    layer_3 = Conv2D(128, (4, 4), (2, 2), activation = lrelu, padding = 'same')(layer_2)\n",
    "    layer_3 = BatchNormalization()(layer_3)\n",
    "    \n",
    "    layer_4 = Conv2D(256, (4, 4), (1, 1), activation = lrelu, padding = 'same')(layer_3)\n",
    "    layer_4 = BatchNormalization()(layer_4)\n",
    "    \n",
    "    layer_4 = Conv2D(1, (4, 4), (1, 1), activation = lrelu, padding = 'same')(layer_4)\n",
    "    \n",
    "    outputs = Activation('sigmoid')(layer_4)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78610cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9690fe",
   "metadata": {},
   "source": [
    "# Loss and Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_tv_loss(image):\n",
    "    loss_y = tf.nn.l2_loss(image[:, 1:, :, :] - image[:, :-1, :, :])\n",
    "    loss_x = tf.nn.l2_loss(image[:, :, 1:, :] - image[:, :, :-1, :])\n",
    "    loss = 2 * (loss_y + loss_x)\n",
    "    loss = tf.cast(loss, tf.float32)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_loss(image, vgg):\n",
    "    vgg.build(image)\n",
    "    loss = vgg.conv3_3\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(predict_real, predict_fake):\n",
    "    return tf.reduce_mean(-(tf.log(predict_real + 1e-12) + tf.log(1 - predict_fake + 1e-12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(predict_fake, targets, outputs, net1, net2):\n",
    "    gen_loss_GAN = -tf.reduce_mean(predict_fake)\n",
    "    gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
    "    gen_loss_tv = tf.reduce_mean(tf.sqrt(tf.nn.l2_loss(sum_tv_loss(outputs))))\n",
    "    gen_loss_f = tf.reduce_mean(tf.sqrt(tf.nn.l2_loss(feature_loss(targets,net1) - feature_loss(outputs,net2))))\n",
    "    gen_loss = gen_loss_GAN + (gen_loss_L1 * 10) + (gen_loss_tv * 1e-5) + (gen_loss_f * 1e-4)\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb02d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sketches, images, net1, net2):    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(sketches, training=True)\n",
    "\n",
    "        predict_real = discriminator_real(concatenate(sketches, images, axis = 3), training = True)\n",
    "        predict_fake = discriminator_fake(concatenate(sketches, generated_images, axis = 3), training = True)\n",
    "\n",
    "        gen_loss = generator_loss(predict_fake, images, generated_images, net1, net2)\n",
    "        disc_loss = discriminator_loss(predict_real, predict_fake)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_real.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_real.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_fake.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e33ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    \n",
    "    net1 = Vgg16()\n",
    "    net2 = Vgg16()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for sketch_batch, image_batch in dataset:\n",
    "            train_step(sketch_batch, image_batch, net1, net2)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "    #     display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84de82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4f6ec37",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb144838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    return tf.image.resize_with_pad(image, 256, 256, method = ResizeMethod.BILINEAR, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e855fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(image):\n",
    "    re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f47aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e81fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb6103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b330c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator_real = create_discriminator()\n",
    "discriminator_fake = create_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f05537",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467add1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78841fdc",
   "metadata": {},
   "source": [
    "# Generate and Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50208ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7645dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68027322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv(batch_input, out_channels, stride):\n",
    "#     with tf.variable_scope(\"conv\"):\n",
    "#         in_channels = batch_input.get_shape()[3]\n",
    "#         filter = tf.get_variable(\"filter\", [4, 4, in_channels, out_channels], dtype=tf.float32,\n",
    "#                                  initializer=tf.random_normal_initializer(0, 0.02))\n",
    "#         # [batch, in_height, in_width, in_channels], [filter_width, filter_height, in_channels, out_channels]\n",
    "#         #     => [batch, out_height, out_width, out_channels]\n",
    "#         padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "#         conv = tf.nn.conv2d(padded_input, filter, [1, stride, stride, 1], padding=\"VALID\")\n",
    "#         return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0beea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0afa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f747e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deconv(batch_input, out_channels):\n",
    "#     with tf.variable_scope(\"deconv\"):\n",
    "#         batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]\n",
    "#         filter = tf.get_variable(\"filter\", [4, 4, out_channels, in_channels], dtype=tf.float32,\n",
    "#                                  initializer=tf.random_normal_initializer(0, 0.02))\n",
    "#         # [batch, in_height, in_width, in_channels], [filter_width, filter_height, out_channels, in_channels]\n",
    "#         #     => [batch, out_height, out_width, out_channels]\n",
    "#         conv = tf.nn.conv2d_transpose(batch_input, filter, [batch, in_height * 2, in_width * 2, out_channels],\n",
    "#                                       [1, 2, 2, 1], padding=\"SAME\")\n",
    "#         return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8590a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e30f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ca16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf461de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
