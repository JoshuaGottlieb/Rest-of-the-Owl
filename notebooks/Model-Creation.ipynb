{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c89f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Conv2D, Conv2DTranspose, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import ReLU, Resizing, Rescaling, concatenate\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.summary import scalar, create_file_writer\n",
    "import time\n",
    "import datetime\n",
    "import keras\n",
    "from tensorflow.io import read_file, decode_jpeg\n",
    "from tensorflow import cast, shape\n",
    "from tensorflow.image import resize, resize_with_pad, ResizeMethod\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow import expand_dims\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117d3d2",
   "metadata": {},
   "source": [
    "# Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1aafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, strides, apply_batchnorm = True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    # Make sequential model\n",
    "    result = Sequential()\n",
    "    # Add Conv2D\n",
    "    result.add(Conv2D(filters, size, strides = strides, padding = 'same',\n",
    "                             kernel_initializer = initializer, use_bias = False))\n",
    "\n",
    "    # Optionally add batchnorm\n",
    "    if apply_batchnorm:\n",
    "        result.add(BatchNormalization())\n",
    "\n",
    "    # Add leaky relu\n",
    "    result.add(LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31722a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, strides, apply_dropout = False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Make sequential model\n",
    "    result = Sequential()\n",
    "    # Add deconv layer (conv2dtranspose)\n",
    "    result.add(Conv2DTranspose(filters, size, strides = strides,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer = initializer,\n",
    "                                    use_bias = False))\n",
    "\n",
    "    # Add batchnorm\n",
    "    result.add(BatchNormalization())\n",
    "\n",
    "    # Optionally add dropout\n",
    "    if apply_dropout:\n",
    "        result.add(Dropout(0.5))\n",
    "    \n",
    "    # Add relu\n",
    "    result.add(ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80b2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    inputs = Input(shape = [256, 256, 1])\n",
    "    \n",
    "    # Define downsampling layers\n",
    "    down_stack = [\n",
    "        downsample(64, 4, 2, apply_batchnorm = False),\n",
    "        downsample(128, 4, 2),\n",
    "        downsample(256, 4, 2),\n",
    "        downsample(512, 4, 2),\n",
    "        downsample(512, 4, 2),\n",
    "        downsample(512, 4, 2),\n",
    "        downsample(512, 4, 2),\n",
    "        downsample(512, 4, 2)\n",
    "    ]\n",
    "    \n",
    "    # Define upsampling layers\n",
    "    up_stack = [\n",
    "        upsample(512, 4, 2, apply_dropout = True),\n",
    "        upsample(512, 4, 2, apply_dropout = True),\n",
    "        upsample(512, 4, 2, apply_dropout = True),\n",
    "        upsample(512, 4, 2),\n",
    "        upsample(256, 4, 2),\n",
    "        upsample(128, 4, 2),\n",
    "        upsample(64, 4, 2),\n",
    "    ]\n",
    "    \n",
    "    # Last layer\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(1, 4, strides = 2, padding = 'same',\n",
    "                                           kernel_initializer = initializer, activation = 'tanh')\n",
    "    \n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs = inputs, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b320ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape = [256, 256, 1], name = 'sketch')\n",
    "    tar = tf.keras.layers.Input(shape = [256, 256, 1], name = 'target')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])\n",
    "\n",
    "    down1 = downsample(64, 4, 2, False)(x)\n",
    "    down2 = downsample(128, 4, 2)(down1)\n",
    "    down3 = downsample(256, 4, 2)(down2)\n",
    "    down4 = downsample(512, 4, 1)(down3)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides = 1, kernel_initializer = initializer)(down4)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inp, tar], outputs = last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985121fb",
   "metadata": {},
   "source": [
    "# Classes and Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2cb535",
   "metadata": {},
   "source": [
    "## Autopainter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aba1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_tv_loss(image):\n",
    "    loss_y = tf.nn.l2_loss(image[:, 1:, :, :] - image[:, :-1, :, :])\n",
    "    loss_x = tf.nn.l2_loss(image[:, :, 1:, :] - image[:, :, :-1, :])\n",
    "    loss = 2 * (loss_y + loss_x)\n",
    "    loss = tf.cast(loss, tf.float32)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a84d0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_loss(image, vgg):\n",
    "    model = Model(inputs = vgg.inputs, outputs = vgg.layers[9].output)\n",
    "    img = tf.reshape(image, [image.shape[-3], image.shape[-2], image.shape[-1]])\n",
    "    img = tf.image.grayscale_to_rgb(img)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = expand_dims(img, axis = 0)\n",
    "    img = preprocess_input(img)\n",
    "    feature_maps = model(img)\n",
    "    \n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2c923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_autopainter(disc_real_output, disc_generated_output):\n",
    "    return tf.reduce_mean(-(tf.math.log(disc_real_output + 1e-12) + tf.math.log(1 - disc_generated_output + 1e-12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "695085d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss_autopainter(disc_generated_output, gen_output, target, net):\n",
    "    gen_loss_GAN = -tf.reduce_mean(disc_generated_output)\n",
    "    gen_loss_L1 = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    gen_loss_tv = tf.reduce_mean(tf.sqrt(tf.nn.l2_loss(sum_tv_loss(gen_output))))\n",
    "    gen_loss_f = tf.reduce_mean(tf.sqrt(tf.nn.l2_loss(feature_loss(target, net) - feature_loss(gen_output, net))))\n",
    "    gen_total_loss = gen_loss_GAN + (gen_loss_L1 * 10) + (gen_loss_tv * 1e-5) + (gen_loss_f * 1e-4)\n",
    "    \n",
    "    return gen_total_loss, gen_loss_GAN, gen_loss_L1, gen_loss_tv, gen_loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afcd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_autopainter(input_image, target, step, generator, discriminator,\n",
    "                           gen_optimizer, discrim_optimizer, net, sum_writer):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_loss_GAN, gen_loss_L1, gen_loss_tv, gen_loss_f =\\\n",
    "            generator_loss_autopainter(disc_generated_output, gen_output, target, net)\n",
    "        disc_loss = discriminator_loss_autopainter(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discrim_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with sum_writer.as_default():\n",
    "        scalar('gen_total_loss', gen_total_loss, step = step)\n",
    "        scalar('gen_gan_loss', gen_loss_GAN, step = step)\n",
    "        scalar('gen_l1_loss', gen_loss_L1, step = step)\n",
    "        scalar('gen_tv_loss', gen_loss_tv, step = step)\n",
    "        scalar('gen_f_loss', gen_loss_f, step = step)\n",
    "        scalar('disc_loss', disc_loss, step = step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7bb0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_autopainter(train_ds, test_ds, epochs, batch_size, num_train, generator, discriminator,\n",
    "                gen_optimizer, discrim_optimizer, net, sum_writer, checkpoint_prefix, checkpoint):\n",
    "\n",
    "    example_target, example_input = next(iter(test_ds.take(1)))\n",
    "    start = time.time()\n",
    "    steps = int(np.round_(epochs * batch_size * num_train))\n",
    "\n",
    "    for step, (target, input_image) in train_ds.repeat().take(steps).enumerate():\n",
    "        if step % (batch_size * num_train) == 0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            if step != 0:\n",
    "                print(f'Time taken for epoch: {time.time() - start:.2f} sec\\n')\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            generate_images(generator, example_input, example_target)\n",
    "            print(f'Epoch: {(step // (batch_size * num_train)) + 1}')\n",
    "\n",
    "        train_step_autopainter(input_image, target, step, generator, discriminator,\n",
    "                               gen_optimizer, discrim_optimizer, net, sum_writer)\n",
    "\n",
    "        # Training step\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "        # Save (checkpoint) the model every 25 epochs\n",
    "        if (step + 1) % (25 * batch_size * num_train) == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix + '_epoch_' + (step + 1) % (25 * batch_size * num_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4d87f",
   "metadata": {},
   "source": [
    "## pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss_pix2pix(disc_generated_output, gen_output, target, loss_object):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (100 * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_pix2pix(disc_real_output, disc_generated_output, loss_object):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_pix2pix(input_image, target, step, generator, discriminator, gen_optimizer, discrim_optimizer, loss_obj, sum_writer):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss_pix2pix(disc_generated_output, gen_output, target, loss_obj)\n",
    "        disc_loss = discriminator_loss_pix2pix(disc_real_output, disc_generated_output, loss_obj)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discrim_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with sum_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pix2pix(train_ds, test_ds, epochs, batch_size, num_train, generator, discriminator,\n",
    "                gen_optimizer, discrim_optimizer, loss_obj,\n",
    "                sum_writer, checkpoint_prefix, checkpoint):\n",
    "\n",
    "    example_target, example_input = next(iter(test_ds.take(1)))\n",
    "    start = time.time()\n",
    "    steps = int(np.round_(epochs * batch_size * num_train))\n",
    "\n",
    "    for step, (target, input_image) in train_ds.repeat().take(steps).enumerate():\n",
    "        if step % (batch_size * num_train) == 0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            if step != 0:\n",
    "                print(f'Time taken for epoch: {time.time() - start:.2f} sec\\n')\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            generate_images(generator, example_input, example_target)\n",
    "            print(f'Epoch: {(step // (batch_size * num_train)) + 1}')\n",
    "\n",
    "        train_step_pix2pix(input_image, target, step, generator, discriminator, gen_optimizer, discrim_optimizer, loss_obj, sum_writer)\n",
    "\n",
    "        # Training step\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "        # Save (checkpoint) the model every 25 epochs\n",
    "        if (step + 1) % (25 * batch_size * num_train) == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix + '_epoch_' + (step + 1) % (25 * batch_size * num_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31588dc6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "307ffade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "  # Read and decode an image file to a uint8 tensor\n",
    "    pair = read_file(image_file)\n",
    "    pair = decode_jpeg(pair)\n",
    "    \n",
    "    w = shape(pair)[1]\n",
    "    w = w // 2\n",
    "    \n",
    "    image = pair[:, w:, :]\n",
    "    sketch = pair[:, :w, :]\n",
    "\n",
    "  # Convert both images to float32 tensors\n",
    "    sketch = cast(sketch, tf.float32)\n",
    "    image = cast(image, tf.float32)\n",
    "\n",
    "    return sketch, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16896859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(sketch, image, height, width):\n",
    "    sketch_resized = resize_with_pad(sketch, height, width, method = ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    image_resized = resize_with_pad(image, height, width, method = ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return sketch_resized, image_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5cd3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the images to [0, 1]\n",
    "def normalize(sketch, image):\n",
    "    sketch_scaled = sketch / 255\n",
    "    image_scaled = image / 255\n",
    "\n",
    "    return sketch_scaled, image_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c05df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_sketch(image_file):\n",
    "    sketch, image = load(image_file)\n",
    "    sketch, image = resize(sketch, image, 256, 256)\n",
    "    sketch, image = normalize(sketch, image)\n",
    "\n",
    "    return sketch, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38b2e9",
   "metadata": {},
   "source": [
    "# Checkpointing and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4886a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint_information(model_name, generator, discriminator, gen_optim, discrim_optim, base_path = '..'):\n",
    "    checkpoint_dir = f'{base_path}/checkpoints/{model_name}/training_checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer = gen_optim,\n",
    "                                     discriminator_optimizer = discrim_optim,\n",
    "                                     generator = generator,\n",
    "                                     discriminator = discrim_optim)\n",
    "    \n",
    "    return checkpoint_prefix, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd22635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_object_and_summary_writer(model_name, base_path = '..'):\n",
    "    loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "    summary_writer = tf.summary.create_file_writer(f'{base_path}/logs/{model_name}/fit/'\n",
    "                                                   + datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "\n",
    "    return loss_obj, summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training = True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "#         plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.imshow(display_list[i][:, : , 0], cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, model_name, save = False, base_path = '..'):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training = False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f'{base_path}/logs/{model_name}/images/image_at_epoch_{epoch:04d}.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ab41b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46a98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files('../data/train/*.jpg')\n",
    "train_dataset = train_dataset.map(load_image_and_sketch,\n",
    "                                  num_parallel_calls = tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(1905)\n",
    "train_dataset = train_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38c3a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files('../data/test/*.jpg')\n",
    "test_dataset = test_dataset.map(load_image_and_sketch,\n",
    "                                  num_parallel_calls = tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c88caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_autopainter = create_generator()\n",
    "discriminator_autopainter = create_discriminator()\n",
    "generator_optimizer_autopainter = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer_autopainter = tf.keras.optimizers.Adam(1e-4)\n",
    "net = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c73e2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj_autopainter, summary_writer_autopainter = create_loss_object_and_summary_writer('autopainter')\n",
    "checkpoint_prefix_autopainter, checkpoint_autopainter = create_checkpoint_information('autopainter',\n",
    "                                                    generator_autopainter, discriminator_autopainter,\n",
    "                                                    generator_optimizer_autopainter, discriminator_optimizer_autopainter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "769477f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_autopainter(train_dataset, test_dataset, epochs = 5, batch_size = 1, num_train = 1905,\n",
    "            generator = generator_autopainter, discriminator = discriminator_autopainter,\n",
    "            gen_optimizer = generator_optimizer_autopainter, discrim_optimizer = discriminator_optimizer_autopainter,\n",
    "            net = net, sum_writer = summary_writer_autopainter,\n",
    "            checkpoint_prefix = checkpoint_prefix_autopainter, checkpoint = checkpoint_autopainter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
